{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANU 465 EEG Project\n",
    "## Exploring Brainwaves vs Creative and Analytical Tasks\n",
    "### By: Faith Tan, Emilie Ho, and Pan Tisapramotkul "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "\n",
    "SAMPLING_RATE = 256\n",
    "CREATIVE_DIR = 'dataset/Drawing/'\n",
    "MATH_DIR = 'dataset/Mathematical/'\n",
    "\n",
    "RAW_CHANNEL = ['RAW_AF7', 'RAW_AF8', 'RAW_TP9', 'RAW_TP10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Raw Data Collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get all the files path in the mathematical and creative directories appened in a list\n",
    "math_files = [os.path.join(MATH_DIR, file) for file in os.listdir(MATH_DIR) if os.path.isfile(os.path.join(MATH_DIR, file))]\n",
    "creative_files = [os.path.join(CREATIVE_DIR, file) for file in os.listdir(CREATIVE_DIR) if os.path.isfile(os.path.join(CREATIVE_DIR, file))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a SAMPLE way on how to perform FFT transform from the **raw data in microvolts** collected by MUSE Monitor\n",
    "\n",
    "MUSE Monitor has 4 electrodes (TF9,AF7,AF8,TP10) that collects data as explained here: https://mind-monitor.com/Technical_Manual.php#help_graph_raw\n",
    "\n",
    "The example below perform FFT on **AF7** data collected. \n",
    "\n",
    "After performing FFT, the peaks, amplitude, and area under the curve maybe be considered to use as features for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5120, 44)\n"
     ]
    }
   ],
   "source": [
    "## PLOTTING THE FFT OF THE RAW_AF7 DATA ##\n",
    "from scipy.signal import hilbert\n",
    "from math import floor\n",
    "\n",
    "\n",
    "def divide_data(data, size):\n",
    "    #TODO: Maybe remove a few seconds at the begining and the end ? \n",
    "\n",
    "    # Split the DataFrame into chunks\n",
    "    num_chunks = floor(len(data) / size)\n",
    "    data_list = []  # List to store chunks\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * size\n",
    "        end_idx = start_idx + size\n",
    "        data_list.append(data.iloc[start_idx:end_idx])\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def cleaned_up_data(file : str):\n",
    "    # get a dataframe\n",
    "    dataset = pd.read_csv(file)\n",
    "    \n",
    "    # drop columns and NaN values\n",
    "    dataset.drop(columns=['Battery','Elements'], inplace=True)\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    dataset['Delta_Average'] = ((dataset['Delta_TP9'] + dataset['Delta_TP10'] + dataset['Delta_AF7'] + dataset['Delta_AF8']) / 4) * 100\n",
    "    dataset['Gamma_Average'] = ((dataset['Gamma_TP9'] + dataset['Gamma_TP10'] + dataset['Gamma_AF7'] + dataset['Gamma_AF8']) / 4) * 100\n",
    "    dataset['Theta_Average'] = ((dataset['Theta_TP9'] + dataset['Theta_TP10'] + dataset['Theta_AF7'] + dataset['Theta_AF8']) / 4) * 100\n",
    "    dataset['Alpha_Average'] = ((dataset['Alpha_TP9'] + dataset['Alpha_TP10'] + dataset['Alpha_AF7'] + dataset['Alpha_AF8']) /4) * 100\n",
    "    dataset['Beta_Average'] = ((dataset['Beta_TP9'] + dataset['Beta_TP10'] + dataset['Beta_AF7'] + dataset['Beta_AF8']) / 4 ) * 100\n",
    "    \n",
    "    dataset['delta_smooth_data'] = moving_average(dataset['Delta_Average'].values, 1000)\n",
    "    dataset['delta_envelope'] = np.abs(hilbert(dataset['delta_smooth_data'].values))\n",
    "    return dataset\n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "def get_all_data(file_list: list):\n",
    "    \"\"\"get data from all the files in the directory, \n",
    "    divide it into 20 seconds intervals and return the data\"\"\"\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    size = SAMPLING_RATE * 20\n",
    "\n",
    "    for file in file_list:\n",
    "        dataset = cleaned_up_data(file)\n",
    "        data_list = divide_data(dataset, size=size)\n",
    "        all_data.extend(data_list)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "all_math_data = get_all_data(math_files)\n",
    "all_drawing_data = get_all_data(creative_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the peaks' amplitude, frequency, and area under the curve from the raw data collected by the four electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "from numpy import trapezoid #type: ignore\n",
    "\n",
    "def get_fft(dataset, channel : str):\n",
    "    \"\"\"Perform FFT on the data of the channel specfied from the range of 0.5 to 50 Hz\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): _description_\n",
    "        channel (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    dataset = dataset[channel]\n",
    "\n",
    "    # Perform FFT \n",
    "    n = len(dataset)                 # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n/SAMPLING_RATE\n",
    "    frq = k/T                 # two sides frequency range\n",
    "    zz=int(n/2)\n",
    "\n",
    "    freq = frq[range(zz)]           # one side frequency range\n",
    "    Y = np.fft.fft(dataset)/n              # fft computing and normalization\n",
    "    Y = abs(Y[range(zz)])\n",
    "\n",
    "    # get only frequency from 0.5 to 50 Hz\n",
    "    freq_mask = (freq>0.5) & (freq<50)\n",
    "    filtered_freq = freq[freq_mask]\n",
    "    filtered_Y = Y[freq_mask]\n",
    "    \n",
    "    return filtered_freq, filtered_Y\n",
    "    \n",
    "def get_peaks(filtered_freq, filtered_Y):\n",
    "    if max(filtered_freq) > 60:\n",
    "        print(\"Warning: Detected frequency Higher than 60 Hz. Please remove it before continuing\")\n",
    "        return \n",
    "    \n",
    "    # Get the peaks\n",
    "    peaks, properties = find_peaks(filtered_Y, height=10)\n",
    "    peak_heights = properties['peak_heights']\n",
    "\n",
    "    # Get top 5 peaks\n",
    "    if len(peaks) > 5:\n",
    "        top_5_indices = np.argsort(peak_heights)[-5:][::-1]  # Indices of top 5 peaks\n",
    "        peaks = peaks[top_5_indices]\n",
    "        peak_heights = peak_heights[top_5_indices]\n",
    "    \n",
    "    # Sort the top 5 peaks in chronological order (by their indices)\n",
    "    sorted_indices = np.argsort(peaks)\n",
    "    peaks = peaks[sorted_indices]\n",
    "    peak_heights = peak_heights[sorted_indices]\n",
    "\n",
    "    return [filtered_freq[i] for i in peaks], peak_heights\n",
    "\n",
    "def get_fft_area(freq, filtered_Y):\n",
    "    alpha_mask = (freq>8) & (freq<13)\n",
    "    beta_mask = (freq>13) & (freq<30)\n",
    "    theta_mask = (freq>4) & (freq<8)\n",
    "    delta_mask = (freq>0.5) & (freq<4)\n",
    "    gamma_mask = (freq>30) & (freq<50)\n",
    "\n",
    "    frequency_masks = [alpha_mask, beta_mask, theta_mask, delta_mask, gamma_mask]\n",
    "\n",
    "    return [trapezoid(filtered_Y[mask], dx=1) for mask in frequency_masks]\n",
    "\n",
    "\n",
    "def find_max_electrode(data: pd.DataFrame):\n",
    "    mean_dict = {\n",
    "        'RAW_AF7': float(data['RAW_AF7'].abs().max() - data['RAW_AF7'].abs().min()), \n",
    "        'RAW_AF8': float(data['RAW_AF8'].abs().max() - data['RAW_AF8'].abs().min()), \n",
    "        'RAW_TP9': float(data['RAW_TP9'].abs().max() - data['RAW_TP9'].abs().min()), \n",
    "        'RAW_TP10': float(data['RAW_TP10'].abs().max() - data['RAW_TP10'].abs().min())\n",
    "        }\n",
    "    \n",
    "    return max(mean_dict, key=mean_dict.get) \n",
    "\n",
    "def feature_extraction(all_data):\n",
    "    feature_data = []\n",
    "\n",
    "    # for each file \n",
    "    for i, data in enumerate(all_data):\n",
    "        # initialize lists\n",
    "        peaks_list = []\n",
    "        amplitude_list = []\n",
    "        area_list = []\n",
    "\n",
    "        # extract data from electrodes USING MAX ELECTRODE RAW DATA\n",
    "        channel = find_max_electrode(data)\n",
    "        freq, y = get_fft(data, channel)\n",
    "        peaks, peak_heights = get_peaks(freq, y) # type: ignore\n",
    "        area = get_fft_area(freq, y)\n",
    "\n",
    "        peaks_list.extend(peaks)\n",
    "        amplitude_list.extend(peak_heights)\n",
    "        area_list.extend(area)\n",
    "\n",
    "        data_info = peaks_list + amplitude_list + area_list\n",
    "        feature_data.append(data_info)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    feature_data = pd.DataFrame(feature_data)\n",
    "\n",
    "    return feature_data\n",
    "\n",
    "def plot_samples():\n",
    "    pass\n",
    "\n",
    "all_features_math = feature_extraction(all_math_data)\n",
    "all_features_drawing = feature_extraction(all_drawing_data)\n",
    "\n",
    "# Add labels to the data\n",
    "all_features_math['label'] = 0\n",
    "all_features_drawing['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envolope the Alpha, Beta, Theta, Gamma Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dateset into the Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# classifier = SVC(kernel = 'rbf', random_state = 0) // try differernals and remove the random_state maybe\n",
    "# classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(cm)\n",
    "# print(\"Your Model Accuracy is=\", accuracy_score(y_test, y_pred)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM: Prediction a new Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (ANN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
