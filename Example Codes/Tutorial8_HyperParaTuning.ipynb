{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151a3ea8",
   "metadata": {},
   "source": [
    "# Tutorial 8: Tuning Hyperparameters for Optimal Model Performance\n",
    "\n",
    "\n",
    "In this tutorial, we will discuss **hyperparameter tuning**, a process in machine learning and deep learning that can  enhance model performance. Hyperparameters are the settings of a model that you configure before training, such as the number of trees in a random forest. Choosing the right hyperparameter values is often the difference between a poor model and an effective one.\n",
    "\n",
    "This session provides an approach to understanding the process of hyperparameter tuning, the methods available, and practice exampled to apply them. By the end, you will be equipped with techniques like **Grid Search** and **Random Search**, and you'll also gain insights into advanced approaches for optimization.\n",
    "\n",
    "---\n",
    "\n",
    "**Importance of Hyperparameter Tuning**\n",
    "Hyperparameter tuning is essential because:\n",
    "- **Improved Model Accuracy:** Properly tuned hyperparameters can drastically improve model accuracy and generalization.\n",
    "- **Reduced Overfitting/Underfitting:** Fine-tuning helps balance model complexity, avoiding both overfitting (too complex) and underfitting (too simple).\n",
    "- **Better Generalization:** Well-tuned hyperparameters ensure the model performs well on unseen data, which is the ultimate goal of machine learning.\n",
    "\n",
    "Hyperparameter tuning is not just about achieving the best possible accuracy but about making informed decisions that result in robust, reliable models.\n",
    "\n",
    "---\n",
    "\n",
    "**Session Outline**\n",
    "This tutorial is divided into two sections:\n",
    "\n",
    "\n",
    "**Section 1: Cross-Validation and Its Role in Tuning**\n",
    "In this section, we’ll discuss:\n",
    "- The **importance of cross-validation** in hyperparameter tuning.\n",
    "- Why **test sets should not be used** during the tuning process.\n",
    "- **Data leakage prevention**:\n",
    "  - When tuning hyperparameters using the test set, we introduce bias because the chosen hyperparameters are influenced by test performance.\n",
    "  - Cross-validation allows us to evaluate models on multiple splits of the data, ensuring that the test set remains unseen until the final evaluation.\n",
    "- Types of cross-validation methods:\n",
    "  - Hold-out validation.\n",
    "  - K-Fold cross-validation.\n",
    "  - Stratified K-Fold cross-validation.\n",
    "  - Leave-One-Out Cross-Validation.\n",
    "\n",
    "This section will emphasize the need for unbiased evaluation of models and highlight how cross-validation ensures that we select the best model while preserving the integrity of the test set.\n",
    "\n",
    "---\n",
    "\n",
    "**Section 2: Hyperparameter Tuning Algorithms**\n",
    "In this section, we’ll explore:\n",
    "- **Basic Tuning Algorithms:**\n",
    "  - **Grid Search:** An exhaustive search over a predefined set of hyperparameters.\n",
    "  - **Random Search:** A more efficient method that randomly samples hyperparameter values.\n",
    "  - **Nested Grid and Random Search:** A hybrid approach where critical hyperparameters are tuned with Grid Search and less critical ones with Random Search.\n",
    "\n",
    "- **Regular Machine Learning Models:**\n",
    "  - Applying Grid and Random Search to common models like Logistic Regression, Random Forest, Support Vector Machine (SVM), and XGBoost.\n",
    "\n",
    "- **Neural Networks:**\n",
    "  - Tuning hyperparameters of neural networks using Grid Search and Random Search.\n",
    "  - Defining architecture and training parameters such as the number of layers, neurons, activation functions, learning rate, and batch size.\n",
    "\n",
    "---\n",
    "\n",
    "**Advanced Techniques (Introduction)**\n",
    "While this tutorial focuses on basic techniques, there advanced methods such as:\n",
    "- Bayesian Optimization\n",
    "- Tree-structured Parzen Estimators (TPE)\n",
    "- Hyperband\n",
    "- Evolutionary Algorithms\n",
    "- Automated Machine Learning (AutoML)\n",
    "\n",
    "---\n",
    "\n",
    "**What You Will Learn**\n",
    "1. The importance of cross-validation in model evaluation and hyperparameter tuning.\n",
    "2. Practical implementation of Grid Search and Random Search for regular models.\n",
    "3. How to tune hyperparameters of neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s get started!\n",
    "\n",
    "We will use the Titanic dataset that was preprocessed in the first session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f4648",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cf9ed-3af2-426e-a4c0-f700e7a9e729",
   "metadata": {
    "id": "681cf9ed-3af2-426e-a4c0-f700e7a9e729"
   },
   "source": [
    "### Importing Some Basic Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50435b6a-111b-495e-9af9-ca2aa0c43bb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1695067288299,
     "user": {
      "displayName": "pegah pourabdollah",
      "userId": "00142689939576443123"
     },
     "user_tz": 420
    },
    "id": "50435b6a-111b-495e-9af9-ca2aa0c43bb3"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # for doing numerical operations\n",
    "import pandas as pd  # for data analysis\n",
    "import matplotlib.pyplot as plt  # for data visualization\n",
    "import seaborn as sns  # for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cf566-b637-4a74-893a-4e88e813b96d",
   "metadata": {
    "id": "d65cf566-b637-4a74-893a-4e88e813b96d"
   },
   "source": [
    "### Importing the Dataset, reading CSV file to a Pandas Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d47113-cba9-49e0-8240-afacc0e46774",
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1695067288301,
     "user": {
      "displayName": "pegah pourabdollah",
      "userId": "00142689939576443123"
     },
     "user_tz": 420
    },
    "id": "95d47113-cba9-49e0-8240-afacc0e46774"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of Siblings/Spouses Aboard</th>\n",
       "      <th>Number of Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.28</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.92</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.10</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  Number of Siblings/Spouses Aboard  \\\n",
       "0    male 22.00                                  1   \n",
       "1  female 38.00                                  1   \n",
       "2  female 26.00                                  0   \n",
       "3  female 35.00                                  1   \n",
       "4    male 35.00                                  0   \n",
       "\n",
       "   Number of Parents/Children Aboard  Fare Embarked  Survived  \n",
       "0                                  0  7.25        S         0  \n",
       "1                                  0 71.28        C         1  \n",
       "2                                  0  7.92        S         1  \n",
       "3                                  0 53.10        S         1  \n",
       "4                                  0  8.05        S         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Titanic_Data.csv')\n",
    "\n",
    "# Set the float format to 2 decimal\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Display the first few rows as sample\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed9f0e-cfb9-4b9c-ab58-fbd28c313198",
   "metadata": {
    "id": "a8ed9f0e-cfb9-4b9c-ab58-fbd28c313198"
   },
   "source": [
    "### Dropping Irrelevant Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "n2lz60YCD3M8",
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1695067289038,
     "user": {
      "displayName": "pegah pourabdollah",
      "userId": "00142689939576443123"
     },
     "user_tz": 420
    },
    "id": "n2lz60YCD3M8"
   },
   "outputs": [],
   "source": [
    "dataset.drop(['PassengerId', 'Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71613ba-1be3-46f5-9d35-ec8bb569e8ed",
   "metadata": {
    "id": "e71613ba-1be3-46f5-9d35-ec8bb569e8ed"
   },
   "source": [
    "### Taking Care of Missing Embarked Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f761f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the 'Embarked' column has missing values\n",
    "dataset = dataset.dropna(subset=['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8655e-b265-43b8-a39d-9fe23a31d1c6",
   "metadata": {
    "id": "1ca8655e-b265-43b8-a39d-9fe23a31d1c6"
   },
   "source": [
    "### Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183b7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for 'Sex' and 'Embarked' columns\n",
    "dataset = pd.get_dummies(dataset, columns=['Sex', 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e1MF69D07i",
   "metadata": {
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1695067289035,
     "user": {
      "displayName": "pegah pourabdollah",
      "userId": "00142689939576443123"
     },
     "user_tz": 420
    },
    "id": "79e1MF69D07i"
   },
   "outputs": [],
   "source": [
    "# Label encoding for 'Survived' column\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset['Survived'] = LabelEncoder().fit_transform(dataset['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bqDJ2nsbOCrZ",
   "metadata": {
    "id": "bqDJ2nsbOCrZ"
   },
   "source": [
    "### Seperate the input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "u8HpDM2ED6-X",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1695067289039,
     "user": {
      "displayName": "pegah pourabdollah",
      "userId": "00142689939576443123"
     },
     "user_tz": 420
    },
    "id": "u8HpDM2ED6-X"
   },
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, 5]\n",
    "X = dataset.drop(['Survived'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3158a8-d49a-44b9-bd32-0c5dbf55b680",
   "metadata": {
    "id": "5a3158a8-d49a-44b9-bd32-0c5dbf55b680"
   },
   "source": [
    "### Splitting the Dataset into the Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hPtdZ9OcEErL",
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1695067289554,
     "user": {
      "displayName": "pegah pourabdollah",
      "userId": "00142689939576443123"
     },
     "user_tz": 420
    },
    "id": "hPtdZ9OcEErL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577cefa",
   "metadata": {},
   "source": [
    "### Taking Care of Missing Age Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "JR8mdGQZEAhd",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1695067289040,
     "user": {
      "displayName": "pegah pourabdollah",
      "userId": "00142689939576443123"
     },
     "user_tz": 420
    },
    "id": "JR8mdGQZEAhd"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732945b-2f59-4d5b-9da3-25ab66fd8d75",
   "metadata": {
    "id": "7732945b-2f59-4d5b-9da3-25ab66fd8d75"
   },
   "source": [
    "### Scaling the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b662ae-42ab-4c1d-bcf7-2d77ddeb5da9",
   "metadata": {
    "code_folding": [],
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1695067289557,
     "user": {
      "displayName": "pegah pourabdollah",
      "userId": "00142689939576443123"
     },
     "user_tz": 420
    },
    "id": "d9b662ae-42ab-4c1d-bcf7-2d77ddeb5da9"
   },
   "outputs": [],
   "source": [
    "# Scale the age and fare\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# `Fit` and `transform` are only applied to the training data\n",
    "X_train[:, [1,4]] = sc.fit_transform(X_train[:, [1,4]])\n",
    "\n",
    "# Just `Transform` is used on the test data\n",
    "X_test[:, [1,4]] = sc.transform(X_test[:, [1,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1690042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both X_train and y_train are NumPy arrays\n",
    "\n",
    "X_train = np.array(X_train)  \n",
    "y_train = np.array(y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeab013",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03cfd6",
   "metadata": {},
   "source": [
    "Cross-validation is crucial in machine learning because it ensures a reliable and unbiased evaluation of model performance during the training and hyperparameter tuning processes. \n",
    "\n",
    "\n",
    "<img src=\"grid_search_cross_validation.png\" alt=\"Cross-validation vs. Test\" width=\"600\">\n",
    "\n",
    "<img src=\"1_lOZqYqwmuW1lg6fitwqXxA.png\" alt=\"Cross-validation performance calculation\" width=\"600\">\n",
    "\n",
    "\n",
    "---\n",
    "Here's why cross-validation is important:\n",
    "\n",
    "1. **Prevents Data Leakage**\n",
    "- The **test set** represents unseen data and should only be used once—at the very end of the modeling process—to evaluate the final model.\n",
    "- If you use the test set during hyperparameter tuning, the model selection process becomes biased toward the test set. This leads to **data leakage**, where the model inadvertently \"learns\" about the test data, making the evaluation unreliable.\n",
    "\n",
    "2. **Reliable Performance Estimates**\n",
    "- Cross-validation splits the data into multiple training and validation sets, ensuring that every data point is used for both training and validation at some point.\n",
    "- This reduces the variability associated with a single train-validation split, leading to a more robust estimate of model performance.\n",
    "\n",
    "3. **Ensures Generalization**\n",
    "- Cross-validation evaluates the model on multiple validation sets, simulating its behavior on unseen data. This helps ensure the model generalizes well to real-world scenarios.\n",
    "- It avoids overfitting to a single validation set, where the model might perform well just because it happens to align with the validation split.\n",
    "\n",
    "4. **Maximizes Data Utilization**\n",
    "- By splitting the data into training and validation sets multiple times (e.g., in K-Fold Cross-Validation), cross-validation makes efficient use of the dataset, especially when it's small.\n",
    "- Every data point is eventually used for training and validation, leading to a more comprehensive evaluation.\n",
    "\n",
    "5. **Model Comparison and Selection**\n",
    "- When tuning hyperparameters or comparing multiple models, cross-validation ensures that the selection process is not biased toward a particular data split.\n",
    "- It helps you confidently choose the model that performs best across different validation sets, rather than one that performs well on a specific split.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Not Use the Test Set for Tuning?**\n",
    "\n",
    "1. **The Role of the Test Set**\n",
    "- The **test set** is a proxy for real-world data, used to measure how well the final model generalizes to unseen scenarios.\n",
    "- Using it during training or hyperparameter tuning contaminates this \"unseen\" nature, making the evaluation meaningless.\n",
    "\n",
    "2. **Risk of Overfitting to the Test Set**\n",
    "- When you tune hyperparameters based on test set performance, you’re inadvertently optimizing the model for that specific test set.\n",
    "- This results in overfitting to the test set and gives a false sense of confidence in the model's ability to handle truly unseen data.\n",
    "\n",
    "3. **Unrealistic Expectations**\n",
    "- In real-world scenarios, you won’t know the characteristics of the test data in advance. Using the test set during tuning creates an unrealistic scenario where you \"peek\" at future data.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| **Cross-Validation**                | **Using Test Set**                     |\n",
    "|-------------------------------------|----------------------------------------|\n",
    "| Used for tuning and evaluating models during the training process. | Should only be used for final model evaluation. |\n",
    "| Provides unbiased performance estimates. | Leads to biased results if used repeatedly. |\n",
    "| Prevents data leakage and ensures generalization. | Causes data leakage if used during tuning. |\n",
    "| Simulates performance on unseen data. | Cannot provide a fair evaluation after being used for tuning. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa22231",
   "metadata": {},
   "source": [
    "**Cross-Validation Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695f51c",
   "metadata": {},
   "source": [
    "**1. K-Fold Cross-Validation**\n",
    "- K-Fold Cross-Validation is a method to evaluate a model's performance by splitting the **training** dataset into **K equal-sized subsets** (folds).\n",
    "- The model is trained on \\(K-1\\) folds and tested on the remaining fold. This process is repeated \\(K\\) times, with each fold serving as the test set exactly once.\n",
    "- The final performance metric is the **average of the scores** across all \\(K\\) iterations.\n",
    "\n",
    "**How does it work?**\n",
    "1. Divide the dataset into \\(K\\) parts (folds).\n",
    "2. For each fold:\n",
    "   - Use \\(K-1\\) folds as the training set.\n",
    "   - Use the remaining fold as the test set.\n",
    "3. Calculate a performance metric (e.g., accuracy) for each fold.\n",
    "4. Average the metrics to get the final score.\n",
    "\n",
    "**Advantages\n",
    "- Reduces bias: Each data point is used for both training and testing.\n",
    "- Helps identify overfitting or underfitting by testing on multiple splits.\n",
    "\n",
    "**Disadvantages**\n",
    "- Computational cost: Training the model \\(K\\) times can be expensive, especially for large datasets or complex models.\n",
    "\n",
    "**When to use?**\n",
    "- Use K-Fold Cross-Validation when you want a reliable performance estimate, and computational resources are not a major constraint.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Stratified K-Fold Cross-Validation**\n",
    "- A variation of K-Fold Cross-Validation that ensures **class proportions in each fold** are similar to the original dataset.\n",
    "- Particularly useful for **imbalanced datasets**, where one class may dominate.\n",
    "\n",
    "**How does it work?**\n",
    "1. Divide the dataset into \\(K\\) folds.\n",
    "2. Ensure that each fold has approximately the same proportion of each class as the entire dataset.\n",
    "3. Follow the same training and testing process as K-Fold Cross-Validation.\n",
    "\n",
    "**Advantages**\n",
    "- Maintains the class distribution, which leads to more reliable evaluation metrics, especially for classification tasks with imbalanced data.\n",
    "\n",
    "**Disadvantages**\n",
    "- Slightly more complex to implement (though most modern libraries like Scikit-learn handle it automatically).\n",
    "\n",
    "**When to use?**\n",
    "- Use Stratified K-Fold Cross-Validation for classification problems, especially when the dataset has an **imbalanced class distribution**.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Leave-One-Out Cross-Validation (LOOCV)**\n",
    "- A special case of K-Fold Cross-Validation where \\(K = n\\) (the number of data points in the dataset).\n",
    "- Each iteration uses \\(n-1\\) data points for training and the **single remaining data point** for testing.\n",
    "\n",
    "**How does it work?**\n",
    "1. For each data point:\n",
    "   - Use all other points as the training set.\n",
    "   - Use the current point as the test set.\n",
    "2. Train and evaluate the model \\(n\\) times.\n",
    "3. Average the results of all \\(n\\) iterations to get the final performance metric.\n",
    "\n",
    "**Advantages**\n",
    "- No randomness: Every data point is used for testing exactly once.\n",
    "- Ideal for very small datasets, as it maximizes the amount of data used for training.\n",
    "\n",
    "**Disadvantages**\n",
    "- Extremely computationally expensive for large datasets, as the model must be trained \\(n\\) times.\n",
    "- May lead to high variance in the evaluation metric because each test set contains only one data point.\n",
    "\n",
    "**When to use?**\n",
    "- Use LOOCV for very **small datasets** where you want the most accurate estimate of model performance.\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison of Techniques**\n",
    "| **Technique**           | **Best For**                 | **Computational Cost**  | **Key Feature**                                   |\n",
    "|--------------------------|------------------------------|--------------------------|--------------------------------------------------|\n",
    "| K-Fold Cross-Validation  | General use cases            | Medium                   | Divides data into \\(K\\) folds.                  |\n",
    "| Stratified K-Fold        | Imbalanced classification    | Medium                   | Preserves class proportions in each fold.       |\n",
    "| LOOCV                   | Small datasets               | High                     | Uses every point for testing exactly once.      |\n",
    "\n",
    "\n",
    "**4. Hold-Out Validation?**\n",
    "- You take a portion of your data (e.g., 80%) for training the model and reserve the remaining portion (e.g., 20%) as a validation set.\n",
    "- The model is trained on the training data and evaluated on the validation set to estimate its performance.\n",
    "\n",
    "**How Does It Work?**\n",
    "1. Split the Data:\n",
    "   - Randomly divide the dataset into two subsets: training and validation.\n",
    "   - For example:\n",
    "     - Training set: 80% of the data.\n",
    "     - Validation set: 20% of the data.\n",
    "\n",
    "2. Train the Model:\n",
    "   - Use the training set to fit the model.\n",
    "\n",
    "3. Evaluate on Validation Set:\n",
    "   - Use the validation set to compute performance metrics (e.g., accuracy, precision, recall, etc.).\n",
    "\n",
    "**Advantages**\n",
    "- Simple and fast: Only one split, so it’s computationally efficient.\n",
    "- No need for complex logic: Easy to implement with minimal coding.\n",
    "\n",
    "**Disadvantages**\n",
    "- Risk of bias: The performance metric might depend on how the data is split. A poorly chosen split might not represent the true performance of the model.\n",
    "- Inefficient use of data: Not all data points contribute to both training and validation, which can be problematic for small datasets.\n",
    "- High variance: Results can vary significantly based on the random split.\n",
    "\n",
    "\n",
    "**When to Use It?**\n",
    "- When you have a large dataset and can afford to reserve a portion for validation without worrying about losing valuable training data.\n",
    "- When you need quick, approximate results and don’t want the overhead of K-Fold or other advanced methods.\n",
    "\n",
    "\n",
    "**Comparison with K-Fold Cross-Validation**\n",
    "| **Hold-Out Validation**               | **K-Fold Cross-Validation**           |\n",
    "|---------------------------------------|---------------------------------------|\n",
    "| Simple and quick to implement         | More complex but robust               |\n",
    "| May introduce bias or high variance   | Reduces bias and variance             |\n",
    "| Efficient for large datasets          | Better for small or medium datasets   |\n",
    "| Less computational overhead           | More computationally expensive        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c721d5",
   "metadata": {},
   "source": [
    "### Hold-Out Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f95d527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-Out Validation Accuracy: 0.7832167832167832\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store test accuracies for each method\n",
    "results = {}\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train_cv, X_val_cv, y_train_cv, y_val_cv = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "# Validate model\n",
    "y_val_pred = model.predict(X_val_cv)\n",
    "val_accuracy = accuracy_score(y_val_cv, y_val_pred)\n",
    "print(\"Hold-Out Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "results[\"Hold-Out\"] = accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92dce7",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a22bb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Initialize K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_kf_model = None\n",
    "best_kf_val_acc = 0\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_kf, X_val_kf = X_train[train_index], X_train[val_index]\n",
    "    y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model.fit(X_train_kf, y_train_kf)\n",
    "    val_acc = accuracy_score(y_val_kf, model.predict(X_val_kf))\n",
    "    \n",
    "    if val_acc > best_kf_val_acc:\n",
    "        best_kf_val_acc = val_acc\n",
    "        best_kf_model = model  # Save the best model\n",
    "\n",
    "# Predict on test set using the best model\n",
    "kf_test_pred = best_kf_model.predict(X_test)\n",
    "results[\"K-Fold\"] = accuracy_score(y_test, kf_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278def5e",
   "metadata": {},
   "source": [
    "### Stratified K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "069495d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_skf_model = None\n",
    "best_skf_val_acc = 0\n",
    "\n",
    "for train_index, val_index in skf.split(X_train, y_train):\n",
    "    X_train_skf, X_val_skf = X_train[train_index], X_train[val_index]\n",
    "    y_train_skf, y_val_skf = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model.fit(X_train_skf, y_train_skf)\n",
    "    val_acc = accuracy_score(y_val_skf, model.predict(X_val_skf))\n",
    "    \n",
    "    if val_acc > best_skf_val_acc:\n",
    "        best_skf_val_acc = val_acc\n",
    "        best_skf_model = model  # Save the best model\n",
    "\n",
    "# Predict on test set using the best model\n",
    "skf_test_pred = best_skf_model.predict(X_test)\n",
    "results[\"Stratified K-Fold\"] = accuracy_score(y_test, skf_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa384089",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross-Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22d39386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Initialize LOOCV\n",
    "loo = LeaveOneOut()\n",
    "best_loo_model = None\n",
    "best_loo_val_acc = 0\n",
    "\n",
    "for train_index, val_index in loo.split(X_train):\n",
    "    X_train_loo, X_val_loo = X_train[train_index], X_train[val_index]\n",
    "    y_train_loo, y_val_loo = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model.fit(X_train_loo, y_train_loo)\n",
    "    val_acc = accuracy_score(y_val_loo, model.predict(X_val_loo))\n",
    "    \n",
    "    if val_acc > best_loo_val_acc:\n",
    "        best_loo_val_acc = val_acc\n",
    "        best_loo_model = model  # Save the best model\n",
    "\n",
    "# Predict on test set using the best model\n",
    "loo_test_pred = best_loo_model.predict(X_test)\n",
    "results[\"LOOCV\"] = accuracy_score(y_test, loo_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd97f0",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3baad18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cross-Validation Method</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hold-Out</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K-Fold</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stratified K-Fold</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOOCV</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cross-Validation Method  Test Accuracy\n",
       "0                Hold-Out           0.81\n",
       "1                  K-Fold           0.83\n",
       "2       Stratified K-Fold           0.83\n",
       "3                   LOOCV           0.84"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(list(results.items()), columns=[\"Cross-Validation Method\", \"Test Accuracy\"])\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612017c",
   "metadata": {},
   "source": [
    "**Summary of Methods**\n",
    "\n",
    "| **Method**               | **Description**                                                                 | **Use Case**                                                                 |\n",
    "|---------------------------|---------------------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
    "| **Hold-Out Validation**   | Simple train-validation split.                                                  | Fast and suitable for large datasets.                                       |\n",
    "| **K-Fold Cross-Validation** | Splits data into \\(k\\) folds, each used as validation set once.                 | General-purpose, reliable for most datasets.                                |\n",
    "| **Stratified K-Fold**     | Similar to K-Fold but preserves class distribution.                             | Best for imbalanced datasets.                                               |\n",
    "| **Leave-One-Out (LOOCV)** | Each sample is used as a validation set once.                                   | Best for small datasets but computationally expensive for large datasets.   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac620fa",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ce235",
   "metadata": {},
   "source": [
    "**Grid Search and Randomized Search**\n",
    "\n",
    "**1. Grid Search**\n",
    "Grid Search is a hyperparameter tuning method that systematically searches through a predefined grid of hyperparameter values. For each combination of parameters, the model is trained, validated, and its performance is recorded.\n",
    "\n",
    "**Advantages**\n",
    "- **Exhaustive Search:** Grid Search explores all possible combinations of hyperparameter values, ensuring that the best combination within the predefined grid is identified.\n",
    "- **Structured:** Easy to understand and implement, especially when the search space is small and clearly defined.\n",
    "- **Deterministic:** Provides consistent results, as it evaluates every possible combination in the grid.\n",
    "\n",
    "**Disadvantages**\n",
    "- **Computationally Expensive:** As the number of hyperparameters and their possible values increases, the search space grows exponentially, leading to a significant computational burden.\n",
    "- **Inefficient for Large Spaces:** Grid Search evaluates combinations that may not contribute significantly to performance improvement, wasting computational resources.\n",
    "- **Limited Exploration:** Restricted to the predefined grid, it might miss optimal values outside the grid.\n",
    "\n",
    "\n",
    "\n",
    "**2. Randomized Search**\n",
    "Randomized Search randomly samples combinations of hyperparameter values from a defined distribution. It evaluates a fixed number of random combinations rather than exploring all possible options.\n",
    "\n",
    "**Advantages**\n",
    "- **Computationally Efficient:** By sampling a fixed number of random combinations, Randomized Search saves time and computational resources.\n",
    "- **Better Coverage in Large Spaces:** Randomized Search can explore a wider range of hyperparameters, potentially identifying better configurations in large or continuous spaces.\n",
    "- **Scalable:** Suitable for problems with many hyperparameters or a large range of values.\n",
    "\n",
    "**Disadvantages**\n",
    "- **Not Exhaustive:** Randomized Search may miss the optimal combination of parameters because it only evaluates a subset of possible values.\n",
    "- **Non-Deterministic:** Results may vary depending on the random seed and number of samples, requiring careful control for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use Both Grid Search and Randomized Search in a Nested Approach?**\n",
    "\n",
    "Combining Grid Search and Randomized Search leverages the strengths of both methods, mitigating their individual limitations:\n",
    "\n",
    "1. **Focused and Efficient Exploration:**\n",
    "   - Use **Grid Search** to exhaustively tune the most critical hyperparameters, which are expected to have the greatest impact on model performance (e.g., learning rate, regularization strength).\n",
    "   - Use **Randomized Search** for less impactful or secondary hyperparameters, allowing a broader but less exhaustive exploration.\n",
    "\n",
    "2. **Balanced Computational Cost:**\n",
    "   - By limiting Grid Search to a small number of critical parameters, the computational overhead is reduced.\n",
    "   - Randomized Search can then explore larger spaces efficiently without sacrificing too much precision.\n",
    "\n",
    "3. **Improved Coverage:**\n",
    "   - Grid Search ensures that critical parameters are thoroughly explored within the specified range.\n",
    "   - Randomized Search introduces stochasticity, helping to identify configurations that might have been missed by the predefined grid.\n",
    "\n",
    "4. **Adaptability:**\n",
    "   - Nested Grid and Randomized Search is adaptable to both small and large datasets or search spaces, providing flexibility in hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "**Example Use Case**\n",
    "For a Random Forest model:\n",
    "- **Grid Search:** Tune `n_estimators` (number of trees) and `max_depth` (tree depth) using a small, predefined grid.\n",
    "- **Randomized Search:** Tune `min_samples_split` and `min_samples_leaf` with broader, random sampling.\n",
    "\n",
    "For a Neural Network:\n",
    "- **Grid Search:** Focus on architectural parameters like the number of layers and neurons.\n",
    "- **Randomized Search:** Explore training-related parameters like learning rate, batch size, and dropout rates.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table: Grid Search vs. Randomized Search**\n",
    "\n",
    "| **Feature**               | **Grid Search**                          | **Randomized Search**                     |\n",
    "|---------------------------|------------------------------------------|-------------------------------------------|\n",
    "| **Exploration**            | Exhaustive within the grid              | Random sampling in a defined space        |\n",
    "| **Computational Cost**     | High for large grids                    | Lower and more scalable                   |\n",
    "| **Efficiency**             | Inefficient for large or continuous spaces | Efficient for high-dimensional spaces     |\n",
    "| **Coverage**               | Limited to the predefined grid          | Covers a broader range of values          |\n",
    "| **Deterministic**          | Yes                                     | No (depends on random sampling)           |\n",
    "| **Best Use Case**          | Small, critical hyperparameter spaces   | Large, less critical or continuous spaces |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b379c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7c9c4e3",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "953f4f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Grid Search for Logistic Regression...\n",
      "Running Grid Search for Random Forest...\n",
      "Running Grid Search for Support Vector Machine...\n",
      "Running Grid Search for K-Nearest Neighbors...\n",
      "\n",
      "Best Overall Model and Parameters:\n",
      "Model: Random Forest\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Validation Accuracy: 0.8241307987786861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define models and their parameter grids\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = []\n",
    "\n",
    "# Perform Grid Search with 5-Fold Cross-Validation for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running Grid Search for {model_name}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[model_name],\n",
    "        cv=5,  # K-Fold Cross-Validation with k=5\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store results\n",
    "    for params, mean_score in zip(grid_search.cv_results_['params'], grid_search.cv_results_['mean_test_score']):\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Parameters': params,\n",
    "            'Validation Accuracy': mean_score\n",
    "        })\n",
    "\n",
    "#     print(f\"Best Parameters for {model_name}: {grid_search.best_params_}\")\n",
    "#     print(f\"Best Validation Accuracy for {model_name}: {grid_search.best_score_}\")\n",
    "\n",
    "    # Update the best model and parameters\n",
    "    if 'Best_Model' not in locals() or grid_search.best_score_ > Best_Model['Validation Accuracy']:\n",
    "        Best_Model = {\n",
    "            'Model': model_name,\n",
    "            'Parameters': grid_search.best_params_,\n",
    "            'Validation Accuracy': grid_search.best_score_\n",
    "        }\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print overall best model and parameters\n",
    "print(\"\\nBest Overall Model and Parameters:\")\n",
    "print(f\"Model: {Best_Model['Model']}\")\n",
    "print(f\"Parameters: {Best_Model['Parameters']}\")\n",
    "print(f\"Validation Accuracy: {Best_Model['Validation Accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8f6d4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 0.01, 'solver': 'liblinear'}</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 0.01, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 0.1, 'solver': 'liblinear'}</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 0.1, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 1, 'solver': 'liblinear'}</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 1, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 10, 'solver': 'liblinear'}</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 10, 'solver': 'lbfgs'}</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 2, 'n...</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 2, 'n...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 2, 'n...</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 5, 'n...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 5, 'n...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 5, 'n...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 10, '...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 10, '...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': None, 'min_samples_split': 10, '...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 2, 'n_e...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 2, 'n_e...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 2, 'n_e...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 2, 'n_e...</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 2, 'n_e...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 2, 'n_e...</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'max_depth': 20, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>{'C': 0.1, 'kernel': 'linear'}</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>{'C': 0.1, 'kernel': 'rbf'}</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>{'C': 1, 'kernel': 'linear'}</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>{'C': 1, 'kernel': 'rbf'}</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>{'C': 10, 'kernel': 'linear'}</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>{'C': 10, 'kernel': 'rbf'}</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>{'n_neighbors': 3, 'weights': 'uniform'}</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>{'n_neighbors': 3, 'weights': 'distance'}</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>{'n_neighbors': 5, 'weights': 'uniform'}</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>{'n_neighbors': 5, 'weights': 'distance'}</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>{'n_neighbors': 7, 'weights': 'uniform'}</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>{'n_neighbors': 7, 'weights': 'distance'}</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model                                         Parameters  \\\n",
       "0      Logistic Regression                 {'C': 0.01, 'solver': 'liblinear'}   \n",
       "1      Logistic Regression                     {'C': 0.01, 'solver': 'lbfgs'}   \n",
       "2      Logistic Regression                  {'C': 0.1, 'solver': 'liblinear'}   \n",
       "3      Logistic Regression                      {'C': 0.1, 'solver': 'lbfgs'}   \n",
       "4      Logistic Regression                    {'C': 1, 'solver': 'liblinear'}   \n",
       "5      Logistic Regression                        {'C': 1, 'solver': 'lbfgs'}   \n",
       "6      Logistic Regression                   {'C': 10, 'solver': 'liblinear'}   \n",
       "7      Logistic Regression                       {'C': 10, 'solver': 'lbfgs'}   \n",
       "8            Random Forest  {'max_depth': None, 'min_samples_split': 2, 'n...   \n",
       "9            Random Forest  {'max_depth': None, 'min_samples_split': 2, 'n...   \n",
       "10           Random Forest  {'max_depth': None, 'min_samples_split': 2, 'n...   \n",
       "11           Random Forest  {'max_depth': None, 'min_samples_split': 5, 'n...   \n",
       "12           Random Forest  {'max_depth': None, 'min_samples_split': 5, 'n...   \n",
       "13           Random Forest  {'max_depth': None, 'min_samples_split': 5, 'n...   \n",
       "14           Random Forest  {'max_depth': None, 'min_samples_split': 10, '...   \n",
       "15           Random Forest  {'max_depth': None, 'min_samples_split': 10, '...   \n",
       "16           Random Forest  {'max_depth': None, 'min_samples_split': 10, '...   \n",
       "17           Random Forest  {'max_depth': 10, 'min_samples_split': 2, 'n_e...   \n",
       "18           Random Forest  {'max_depth': 10, 'min_samples_split': 2, 'n_e...   \n",
       "19           Random Forest  {'max_depth': 10, 'min_samples_split': 2, 'n_e...   \n",
       "20           Random Forest  {'max_depth': 10, 'min_samples_split': 5, 'n_e...   \n",
       "21           Random Forest  {'max_depth': 10, 'min_samples_split': 5, 'n_e...   \n",
       "22           Random Forest  {'max_depth': 10, 'min_samples_split': 5, 'n_e...   \n",
       "23           Random Forest  {'max_depth': 10, 'min_samples_split': 10, 'n_...   \n",
       "24           Random Forest  {'max_depth': 10, 'min_samples_split': 10, 'n_...   \n",
       "25           Random Forest  {'max_depth': 10, 'min_samples_split': 10, 'n_...   \n",
       "26           Random Forest  {'max_depth': 20, 'min_samples_split': 2, 'n_e...   \n",
       "27           Random Forest  {'max_depth': 20, 'min_samples_split': 2, 'n_e...   \n",
       "28           Random Forest  {'max_depth': 20, 'min_samples_split': 2, 'n_e...   \n",
       "29           Random Forest  {'max_depth': 20, 'min_samples_split': 5, 'n_e...   \n",
       "30           Random Forest  {'max_depth': 20, 'min_samples_split': 5, 'n_e...   \n",
       "31           Random Forest  {'max_depth': 20, 'min_samples_split': 5, 'n_e...   \n",
       "32           Random Forest  {'max_depth': 20, 'min_samples_split': 10, 'n_...   \n",
       "33           Random Forest  {'max_depth': 20, 'min_samples_split': 10, 'n_...   \n",
       "34           Random Forest  {'max_depth': 20, 'min_samples_split': 10, 'n_...   \n",
       "35  Support Vector Machine                     {'C': 0.1, 'kernel': 'linear'}   \n",
       "36  Support Vector Machine                        {'C': 0.1, 'kernel': 'rbf'}   \n",
       "37  Support Vector Machine                       {'C': 1, 'kernel': 'linear'}   \n",
       "38  Support Vector Machine                          {'C': 1, 'kernel': 'rbf'}   \n",
       "39  Support Vector Machine                      {'C': 10, 'kernel': 'linear'}   \n",
       "40  Support Vector Machine                         {'C': 10, 'kernel': 'rbf'}   \n",
       "41     K-Nearest Neighbors           {'n_neighbors': 3, 'weights': 'uniform'}   \n",
       "42     K-Nearest Neighbors          {'n_neighbors': 3, 'weights': 'distance'}   \n",
       "43     K-Nearest Neighbors           {'n_neighbors': 5, 'weights': 'uniform'}   \n",
       "44     K-Nearest Neighbors          {'n_neighbors': 5, 'weights': 'distance'}   \n",
       "45     K-Nearest Neighbors           {'n_neighbors': 7, 'weights': 'uniform'}   \n",
       "46     K-Nearest Neighbors          {'n_neighbors': 7, 'weights': 'distance'}   \n",
       "\n",
       "    Validation Accuracy  \n",
       "0                  0.74  \n",
       "1                  0.75  \n",
       "2                  0.78  \n",
       "3                  0.79  \n",
       "4                  0.79  \n",
       "5                  0.79  \n",
       "6                  0.79  \n",
       "7                  0.79  \n",
       "8                  0.80  \n",
       "9                  0.81  \n",
       "10                 0.80  \n",
       "11                 0.81  \n",
       "12                 0.81  \n",
       "13                 0.81  \n",
       "14                 0.82  \n",
       "15                 0.81  \n",
       "16                 0.81  \n",
       "17                 0.81  \n",
       "18                 0.81  \n",
       "19                 0.81  \n",
       "20                 0.81  \n",
       "21                 0.82  \n",
       "22                 0.82  \n",
       "23                 0.82  \n",
       "24                 0.82  \n",
       "25                 0.82  \n",
       "26                 0.80  \n",
       "27                 0.81  \n",
       "28                 0.80  \n",
       "29                 0.81  \n",
       "30                 0.81  \n",
       "31                 0.81  \n",
       "32                 0.82  \n",
       "33                 0.81  \n",
       "34                 0.81  \n",
       "35                 0.78  \n",
       "36                 0.79  \n",
       "37                 0.77  \n",
       "38                 0.80  \n",
       "39                 0.77  \n",
       "40                 0.82  \n",
       "41                 0.79  \n",
       "42                 0.77  \n",
       "43                 0.80  \n",
       "44                 0.77  \n",
       "45                 0.79  \n",
       "46                 0.77  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a4ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model Performance on Test Set:\n",
      "Model: Random Forest\n",
      "Train Accuracy: 0.8917\n",
      "Test Accuracy: 0.8315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train the best model with the best parameters on the full training data\n",
    "final_model = models[Best_Model['Model']]\n",
    "final_model.set_params(**Best_Model['Parameters'])  # Apply the best parameters\n",
    "final_model.fit(X_train, y_train)  # Train on the full training data\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Evaluate performance on the train set\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nBest Model Performance on Test Set:\")\n",
    "print(f\"Model: {Best_Model['Model']}\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9a84d",
   "metadata": {},
   "source": [
    "### Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c0d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Randomized Search for Logistic Regression...\n",
      "Running Randomized Search for Random Forest...\n",
      "Running Randomized Search for Support Vector Machine...\n",
      "Running Randomized Search for K-Nearest Neighbors...\n",
      "\n",
      "Best Overall Model and Parameters:\n",
      "Model: Random Forest\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 7, 'n_estimators': 179}\n",
      "Validation Accuracy: 0.8241504973899341\n",
      "\n",
      "Best Model Performance on Test Set:\n",
      "Model: Random Forest\n",
      "Train Accuracy: 0.9072\n",
      "Test Accuracy: 0.8371\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "\n",
    "# Define models and their hyperparameter distributions\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "param_distributions = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': uniform(0.01, 10),  # Continuous uniform distribution\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': randint(2, 20)\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        'C': uniform(0.1, 10),  # Continuous uniform distribution\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        'n_neighbors': randint(3, 20),\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = []\n",
    "\n",
    "# Perform Randomized Search with 5-Fold Cross-Validation for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running Randomized Search for {model_name}...\")\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_distributions[model_name],\n",
    "        n_iter=20,  # Number of parameter combinations to try\n",
    "        cv=5,  # K-Fold Cross-Validation with k=5\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store results\n",
    "    for params, mean_score in zip(random_search.cv_results_['params'], random_search.cv_results_['mean_test_score']):\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Parameters': params,\n",
    "            'Validation Accuracy': mean_score\n",
    "        })\n",
    "\n",
    "    # Update the best model and parameters\n",
    "    if 'Best_Model' not in locals() or random_search.best_score_ > Best_Model['Validation Accuracy']:\n",
    "        Best_Model = {\n",
    "            'Model': model_name,\n",
    "            'Parameters': random_search.best_params_,\n",
    "            'Validation Accuracy': random_search.best_score_\n",
    "        }\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print overall best model and parameters\n",
    "print(\"\\nBest Overall Model and Parameters:\")\n",
    "print(f\"Model: {Best_Model['Model']}\")\n",
    "print(f\"Parameters: {Best_Model['Parameters']}\")\n",
    "print(f\"Validation Accuracy: {Best_Model['Validation Accuracy']}\")\n",
    "\n",
    "\n",
    "# Train the best model with the best parameters on the full training data\n",
    "final_model = models[Best_Model['Model']]\n",
    "final_model.set_params(**Best_Model['Parameters'])  # Apply the best parameters\n",
    "final_model.fit(X_train, y_train)  # Train on the full training data\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Evaluate performance on the train set\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nBest Model Performance on Test Set:\")\n",
    "print(f\"Model: {Best_Model['Model']}\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd767c",
   "metadata": {},
   "source": [
    "### Nested Grid and Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "423b6a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Nested Search for Logistic Regression...\n",
      "Running Nested Search for Random Forest...\n",
      "Running Nested Search for Support Vector Machine...\n",
      "Running Nested Search for K-Nearest Neighbors...\n",
      "\n",
      "Best Overall Model and Parameters:\n",
      "Model: Random Forest\n",
      "Parameters: {'max_depth': 10, 'min_samples_split': 7, 'n_estimators': 179}\n",
      "Validation Accuracy: 0.8241504973899341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epasb\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 2 is smaller than n_iter=20. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model Performance on Test Set:\n",
      "Train Accuracy: 0.9072\n",
      "Test Accuracy: 0.8371\n"
     ]
    }
   ],
   "source": [
    "# Define models and their hyperparameters for Grid and Random Search\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "grid_params = {\n",
    "    \"Logistic Regression\": {\n",
    "        'solver': ['liblinear', 'lbfgs'],  # Critical hyperparameter for Grid Search\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [50, 100, 200],  # Critical hyperparameter for Grid Search\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        'kernel': ['linear', 'rbf'],  # Critical hyperparameter for Grid Search\n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        'n_neighbors': [3, 5, 7],  # Critical hyperparameter for Grid Search\n",
    "    }\n",
    "}\n",
    "\n",
    "random_params = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': uniform(0.01, 10),  # Less critical, tuned with Random Search\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'max_depth': [None, 10, 20],  # Less critical\n",
    "        'min_samples_split': randint(2, 20),  # Less critical\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        'C': uniform(0.1, 10),  # Less critical\n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        'weights': ['uniform', 'distance'],  # Less critical\n",
    "    }\n",
    "}\n",
    "\n",
    "random_params = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': uniform(0.01, 10),  # Less critical, tuned with Random Search\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': randint(2, 50),  # Less critical, tuned with Random Search\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        'C': uniform(0.1, 50),  # Less critical, tuned with Random Search\n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        'weights': ['uniform', 'distance'],  # Less critical, tuned with Random Search\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "# Perform Nested Grid and Random Search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running Nested Search for {model_name}...\")\n",
    "    \n",
    "    # Step 1: Perform Grid Search for critical hyperparameters\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=grid_params[model_name],\n",
    "        cv=5,  # K-Fold Cross-Validation with k=5\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_grid_params = grid_search.best_params_\n",
    "    \n",
    "    # Step 2: Perform Random Search for less critical hyperparameters\n",
    "    \n",
    "    # Update model with best parameters from Grid Search\n",
    "    model.set_params(**best_grid_params)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=random_params[model_name],\n",
    "        n_iter=20,  # Number of parameter combinations to try\n",
    "        cv=5,  # K-Fold Cross-Validation with k=5\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_random_params = random_search.best_params_\n",
    "    \n",
    "    # Combine best parameters from Grid and Random Search\n",
    "    combined_best_params = {**best_grid_params, **best_random_params}\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Grid Search Parameters': best_grid_params,\n",
    "        'Random Search Parameters': best_random_params,\n",
    "        'Combined Parameters': combined_best_params,\n",
    "        'Validation Accuracy': random_search.best_score_\n",
    "    })\n",
    "\n",
    "    # Update the best model and parameters\n",
    "    if 'Best_Model' not in locals() or random_search.best_score_ > Best_Model['Validation Accuracy']:\n",
    "        Best_Model = {\n",
    "            'Model': model_name,\n",
    "            'Parameters': combined_best_params,\n",
    "            'Validation Accuracy': random_search.best_score_\n",
    "        }\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print overall best model and parameters\n",
    "print(\"\\nBest Overall Model and Parameters:\")\n",
    "print(f\"Model: {Best_Model['Model']}\")\n",
    "print(f\"Parameters: {Best_Model['Parameters']}\")\n",
    "print(f\"Validation Accuracy: {Best_Model['Validation Accuracy']}\")\n",
    "\n",
    "# Train the best model with the best parameters on the full training data\n",
    "final_model = models[Best_Model['Model']]\n",
    "final_model.set_params(**Best_Model['Parameters'])  # Apply the combined best parameters\n",
    "final_model.fit(X_train, y_train)  # Train on the full training data\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Evaluate performance on the train set\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nBest Model Performance on Test Set:\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ac157",
   "metadata": {},
   "source": [
    "### Nested Grid and Random Search on ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6600b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Grid Search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epasb\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Best Parameters: {'batch_size': 8, 'model__n_layers': 3, 'model__n_neurons': 32}\n",
      "Grid Search Best Validation Accuracy: 0.7988748241912799\n",
      "Running Random Search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epasb\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search Best Parameters: {'epochs': 61, 'model__dropout_rate': 0.4753571532049581, 'model__learning_rate': 0.007419939418114052}\n",
      "Random Search Best Validation Accuracy: 0.80028129395218\n",
      "Combined Best Parameters: {'batch_size': 8, 'model__n_layers': 3, 'model__n_neurons': 32, 'epochs': 61, 'model__dropout_rate': 0.4753571532049581, 'model__learning_rate': 0.007419939418114052}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epasb\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "Best ANN Model Performance:\n",
      "Train Accuracy: 0.8495\n",
      "Test Accuracy: 0.8146\n"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a function to create the ANN model\n",
    "def create_model(n_layers=1, n_neurons=32, dropout_rate=0.0, learning_rate=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer\n",
    "    for _ in range(n_layers - 1):\n",
    "        model.add(Dense(n_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(len(np.unique(y_train)), activation='softmax'))  # Output layer\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',  # Use sparse loss for label-based targets\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model for compatibility with scikit-learn\n",
    "model_wrapper = KerasClassifier(\n",
    "    model=create_model,\n",
    "    verbose=0  # Suppress training logs\n",
    ")\n",
    "\n",
    "# Step 1: Grid Search for critical hyperparameters\n",
    "grid_params = {\n",
    "    'model__n_layers': [1, 2, 3],  # Number of hidden layers\n",
    "    'model__n_neurons': [16, 32],  # Number of neurons per layer\n",
    "    'batch_size': [8, 16]  # Batch size\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_wrapper,\n",
    "    param_grid=grid_params,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "print(\"Running Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from Grid Search\n",
    "best_grid_params = grid_search.best_params_\n",
    "print(f\"Grid Search Best Parameters: {best_grid_params}\")\n",
    "print(f\"Grid Search Best Validation Accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# Step 2: Random Search for less critical hyperparameters\n",
    "# Update model with best parameters from Grid Search\n",
    "model_wrapper.set_params(**best_grid_params)\n",
    "\n",
    "random_params = {\n",
    "    'model__learning_rate': uniform(0.0001, 0.01),  # Learning rate\n",
    "    'model__dropout_rate': uniform(0, 0.5),  # Dropout rates\n",
    "    'epochs': randint(10, 100)  # Integer range for number of epochs\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_wrapper,\n",
    "    param_distributions=random_params,\n",
    "    n_iter=10,  # Number of random combinations to try\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Running Random Search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from Random Search\n",
    "best_random_params = random_search.best_params_\n",
    "print(f\"Random Search Best Parameters: {best_random_params}\")\n",
    "print(f\"Random Search Best Validation Accuracy: {random_search.best_score_}\")\n",
    "\n",
    "# Combine best parameters from both searches\n",
    "combined_best_params = {**best_grid_params, **best_random_params}\n",
    "print(f\"Combined Best Parameters: {combined_best_params}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = create_model(**{\n",
    "    'n_layers': combined_best_params['model__n_layers'],\n",
    "    'n_neurons': combined_best_params['model__n_neurons'],\n",
    "    'dropout_rate': combined_best_params['model__dropout_rate'],\n",
    "    'learning_rate': combined_best_params['model__learning_rate']\n",
    "})\n",
    "\n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=combined_best_params['epochs'],\n",
    "    batch_size=combined_best_params['batch_size'],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict on both train and test sets\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_train_pred = np.argmax(y_train_pred, axis=1)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "# Evaluate performance on train and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nBest ANN Model Performance:\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
