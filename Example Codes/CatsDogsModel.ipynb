{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Cats & Dogs Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "In this classic Machine Learning example, you are given two sets of totally different images. One set is 8,000 images of dogs and another 8,000 images of cats. All these images are in the Folder, Dataset/Training_Set.\n",
    "Your task is to build a model capable of recognizing if an unknown animal is a cat or a dog.<br>\n",
    "There is also a Test_Set, containing two Folders each contains 2,000 images of cats and 2,000 dogs. So, you can validate your model accuracy with this test_set. Later, try to use the model to predict if a single unknown image (some examples in the Folder, New_set) is a dog or a cat.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0koUcJMJpEBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   #shear_range = 0.2,\n",
    "                                   #zoom_range = 0.2,\n",
    "                                   #horizontal_flip = True)\n",
    "#training_set = train_datagen.flow_from_directory('Dataset/Training_Set',\n",
    "                                                 #target_size = (64, 64),\n",
    "                                                 #batch_size = 32,\n",
    "                                                 #class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding More Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,  # Center pixel values\n",
    "    featurewise_std_normalization=True,  # Normalize pixel values\n",
    "    rotation_range=30,  # Rotate up to 30 degrees\n",
    "    width_shift_range=0.2,  # Shift width by 20%\n",
    "    height_shift_range=0.2,  # Shift height by 20%\n",
    "    shear_range=0.2,  # Shear transformation\n",
    "    brightness_range=(0.7, 1.3),  # Adjust brightness\n",
    "    channel_shift_range=30.0,  # Adjust channel shifts\n",
    "    fill_mode='nearest',  # Filling mode\n",
    "    zoom_range=0.2,  # Random zoom\n",
    "    horizontal_flip=True,  # Horizontal flipping\n",
    "    vertical_flip=True  # Vertical flipping\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    directory='Dataset/Training_Set',\n",
    "    target_size=(64, 64),  # Adjust to your image size\n",
    "    batch_size=32,  # Batch size\n",
    "    class_mode='binary',  # Set the appropriate class mode\n",
    "    save_to_dir='Dataset/Augmented_data',  # Directory to save augmented images (optional)\n",
    "    save_prefix='aug',  # Prefix for saved images (optional)\n",
    "    save_format='jpeg'  # Image format for saved images (optional)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SH4WzfOhpKc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('Dataset/Test_Set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of n-classes, class_mode ='categorical'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Building the CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Step 1 - Initialising the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAUt4UMPlhLS"
   },
   "outputs": [],
   "source": [
    "Model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 2 - Adding First Convolution Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPzPrMckl-hV"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 3 - Pooling the First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncpqPl69mOac"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Step 4 - Adding a Second Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_-FZjn_m8gk"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Pooling the Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 6 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AZeOGCvnNZn"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 7 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GtmUlLd26Nq"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 8 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1p_Zj1Mc3Ko_"
   },
   "outputs": [],
   "source": [
    "Model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of n-classes: you need to specify n here:\n",
    "#Model.add(tf.keras.layers.Dense(units=n, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Step 9 - Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NALksrNQpUlJ"
   },
   "outputs": [],
   "source": [
    "Model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of n-classes\n",
    "#Molel.compile(loss='sparse_categorical_crossentropy', optimizer='nadam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Training the CNN and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpanah\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1861: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mpanah\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:1871: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 131s 519ms/step - loss: 0.6830 - accuracy: 0.5456 - val_loss: 0.6546 - val_accuracy: 0.6125\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 39s 158ms/step - loss: 0.6547 - accuracy: 0.6068 - val_loss: 0.6437 - val_accuracy: 0.6075\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.6421 - accuracy: 0.6250 - val_loss: 0.5964 - val_accuracy: 0.6840\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.6278 - accuracy: 0.6444 - val_loss: 0.6263 - val_accuracy: 0.6415\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.6213 - accuracy: 0.6570 - val_loss: 0.5763 - val_accuracy: 0.7055\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.6153 - accuracy: 0.6551 - val_loss: 0.5747 - val_accuracy: 0.7015\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.6012 - accuracy: 0.6695 - val_loss: 0.5625 - val_accuracy: 0.7035\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.6004 - accuracy: 0.6684 - val_loss: 0.5502 - val_accuracy: 0.7120\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.5940 - accuracy: 0.6751 - val_loss: 0.5587 - val_accuracy: 0.7020\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.5854 - accuracy: 0.6842 - val_loss: 0.5294 - val_accuracy: 0.7435\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.5844 - accuracy: 0.6835 - val_loss: 0.5256 - val_accuracy: 0.7450\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 39s 158ms/step - loss: 0.5828 - accuracy: 0.6862 - val_loss: 0.5310 - val_accuracy: 0.7500\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.5769 - accuracy: 0.6934 - val_loss: 0.5256 - val_accuracy: 0.7280\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.5759 - accuracy: 0.6957 - val_loss: 0.5446 - val_accuracy: 0.7210\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 40s 158ms/step - loss: 0.5683 - accuracy: 0.7003 - val_loss: 0.5183 - val_accuracy: 0.7545\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 41s 163ms/step - loss: 0.5696 - accuracy: 0.7041 - val_loss: 0.5448 - val_accuracy: 0.7150\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.5637 - accuracy: 0.7024 - val_loss: 0.5567 - val_accuracy: 0.6910\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.5619 - accuracy: 0.7116 - val_loss: 0.5588 - val_accuracy: 0.6900\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.5618 - accuracy: 0.7010 - val_loss: 0.5075 - val_accuracy: 0.7460\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.5608 - accuracy: 0.7081 - val_loss: 0.5009 - val_accuracy: 0.7465\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 40s 161ms/step - loss: 0.5531 - accuracy: 0.7095 - val_loss: 0.5074 - val_accuracy: 0.7455\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.5539 - accuracy: 0.7164 - val_loss: 0.5037 - val_accuracy: 0.7445\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 40s 162ms/step - loss: 0.5482 - accuracy: 0.7195 - val_loss: 0.5643 - val_accuracy: 0.7095\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.5411 - accuracy: 0.7250 - val_loss: 0.5035 - val_accuracy: 0.7545\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 40s 159ms/step - loss: 0.5435 - accuracy: 0.7271 - val_loss: 0.4847 - val_accuracy: 0.7605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28ea60823d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 3s 53ms/step - loss: 0.4847 - accuracy: 0.7605\n",
      "Accuracy: 76.05%\n"
     ]
    }
   ],
   "source": [
    "accuracy = Model.evaluate(test_set)\n",
    "print(f\"Accuracy: {accuracy[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### And, if we want to plot the Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#history = Model.fit(training_set, validation_data=test_set, epochs=25)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "\n",
    "#plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "#plt.title('Model accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Making a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.preprocessing.image' has no attribute 'load_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[1;32m----> 3\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset/Single_Prediction/Animal_1.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m      4\u001b[0m test_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(test_image)\n\u001b[0;32m      5\u001b[0m test_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.preprocessing.image' has no attribute 'load_img'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('Dataset/Single_Prediction/Animal_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = Model.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
